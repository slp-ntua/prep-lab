{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shell scripting tutorial\n",
    "\n",
    "Learning shell scripting can provide you quick and easy ways to perform a lot of work related with a machine learning / text processing project.\n",
    "\n",
    "Some of the things you can achieve with shell scripting:\n",
    "\n",
    "- Installing project dependencies\n",
    "- Build project dependencies from source\n",
    "- Download organize and clean data\n",
    "- Extract data statistics (e.g. word / character counts)\n",
    "- Perform text processing\n",
    "- Train and evaluate models using existing frameworks that provide a command line interface (Kaldi, openfst, fasttext, fairseq etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all list current working directory and it's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/geopar/projects/prep-lab/bash\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 212K\n",
      "drwxrwxr-x 3 geopar geopar 4.0K Mar 27 19:59  .\n",
      "drwxrwxr-x 6 geopar geopar 4.0K Mar 27 19:43  ..\n",
      "drwxrwxr-x 2 geopar geopar 4.0K Mar 27 19:59  .ipynb_checkpoints\n",
      "-rw-rw-r-- 1 geopar geopar  69K Mar 27 19:43 'Bash For Text Processing Introduction.ipynb'\n",
      "-rw-rw-r-- 1 geopar geopar 1.6K Mar 27 19:43  README.md\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:43  auto.csv\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:48  auto1.csv\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:48  auto2.csv\n",
      "-rw-rw-r-- 1 geopar geopar 2.1K Mar 27 19:43  basic-commands.sh\n",
      "-rwxrwxr-x 1 geopar geopar  543 Mar 27 19:43  chcase.py\n",
      "-rw-rw-r-- 1 geopar geopar 1.8K Mar 27 19:55  file-commands.sh\n",
      "-rw-r--r-- 1 geopar geopar  13K Mar 27 19:44  h_kernel.install\n",
      "-rw-rw-r-- 1 geopar geopar  454 Mar 27 19:43  install-openfst.sh\n",
      "-rw-rw-r-- 1 geopar geopar  535 Mar 27 19:43  install_openfst.sh\n",
      "-rw-rw-r-- 1 geopar geopar  440 Mar 27 19:43  system-commands.sh\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing project dependencies and building a project from source\n",
    "\n",
    "Let's say we need to build an N-Gram language model for some corpus. One commonly used tool for this is KenLM. Let's download and build it from source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download KenLM from git repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'kenlm'...\n",
      "remote: Enumerating objects: 14142, done.\u001b[K\n",
      "remote: Counting objects: 100% (455/455), done.\u001b[K\n",
      "remote: Compressing objects: 100% (318/318), done.\u001b[K\n",
      "remote: Total 14142 (delta 149), reused 393 (delta 123), pack-reused 13687\u001b[K\n",
      "Receiving objects: 100% (14142/14142), 5.91 MiB | 13.77 MiB/s, done.\n",
      "Resolving deltas: 100% (8029/8029), done.\n"
     ]
    }
   ],
   "source": [
    "git clone https://github.com/kpu/kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install necessary dependencies for building KenLM (follow docs: https://kheafield.com/code/kenlm/dependencies/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get install build-essential libboost-all-dev cmake zlib1g-dev libbz2-dev liblzma-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List files in current directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 216K\n",
      "drwxrwxr-x 4 geopar geopar 4.0K Mar 27 20:00  .\n",
      "drwxrwxr-x 6 geopar geopar 4.0K Mar 27 19:43  ..\n",
      "drwxrwxr-x 2 geopar geopar 4.0K Mar 27 19:59  .ipynb_checkpoints\n",
      "-rw-rw-r-- 1 geopar geopar  69K Mar 27 19:43 'Bash For Text Processing Introduction.ipynb'\n",
      "-rw-rw-r-- 1 geopar geopar 1.6K Mar 27 19:43  README.md\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:43  auto.csv\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:48  auto1.csv\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:48  auto2.csv\n",
      "-rw-rw-r-- 1 geopar geopar 2.1K Mar 27 19:43  basic-commands.sh\n",
      "-rwxrwxr-x 1 geopar geopar  543 Mar 27 19:43  chcase.py\n",
      "-rw-rw-r-- 1 geopar geopar 1.8K Mar 27 19:55  file-commands.sh\n",
      "-rw-r--r-- 1 geopar geopar  13K Mar 27 19:44  h_kernel.install\n",
      "-rw-rw-r-- 1 geopar geopar  454 Mar 27 19:43  install-openfst.sh\n",
      "-rw-rw-r-- 1 geopar geopar  535 Mar 27 19:43  install_openfst.sh\n",
      "drwxrwxr-x 8 geopar geopar 4.0K Mar 27 20:00  kenlm\n",
      "-rw-rw-r-- 1 geopar geopar  440 Mar 27 19:43  system-commands.sh\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate inside kenlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd kenlm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/geopar/projects/prep-lab/bash/kenlm\n"
     ]
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 220K\n",
      "drwxrwxr-x 8 geopar geopar 4.0K Mar 27 20:00 .\n",
      "drwxrwxr-x 4 geopar geopar 4.0K Mar 27 20:00 ..\n",
      "drwxrwxr-x 6 geopar geopar 4.0K Mar 27 20:00 .git\n",
      "drwxrwxr-x 3 geopar geopar 4.0K Mar 27 20:00 .github\n",
      "-rw-rw-r-- 1 geopar geopar  261 Mar 27 20:00 .gitignore\n",
      "-rw-rw-r-- 1 geopar geopar  696 Mar 27 20:00 BUILDING\n",
      "-rw-rw-r-- 1 geopar geopar 4.7K Mar 27 20:00 CMakeLists.txt\n",
      "-rw-rw-r-- 1 geopar geopar  26K Mar 27 20:00 COPYING\n",
      "-rw-rw-r-- 1 geopar geopar  35K Mar 27 20:00 COPYING.3\n",
      "-rw-rw-r-- 1 geopar geopar 7.5K Mar 27 20:00 COPYING.LESSER.3\n",
      "-rw-rw-r-- 1 geopar geopar  63K Mar 27 20:00 Doxyfile\n",
      "-rw-rw-r-- 1 geopar geopar 1.2K Mar 27 20:00 LICENSE\n",
      "-rw-rw-r-- 1 geopar geopar  220 Mar 27 20:00 MANIFEST.in\n",
      "-rw-rw-r-- 1 geopar geopar 5.9K Mar 27 20:00 README.md\n",
      "-rwxrwxr-x 1 geopar geopar   81 Mar 27 20:00 clean_query_only.sh\n",
      "drwxrwxr-x 3 geopar geopar 4.0K Mar 27 20:00 cmake\n",
      "-rwxrwxr-x 1 geopar geopar 1.2K Mar 27 20:00 compile_query_only.sh\n",
      "drwxrwxr-x 7 geopar geopar 4.0K Mar 27 20:00 lm\n",
      "-rw-rw-r-- 1 geopar geopar   59 Mar 27 20:00 pyproject.toml\n",
      "drwxrwxr-x 2 geopar geopar 4.0K Mar 27 20:00 python\n",
      "-rw-rw-r-- 1 geopar geopar 4.3K Mar 27 20:00 setup.py\n",
      "drwxrwxr-x 4 geopar geopar 4.0K Mar 27 20:00 util\n"
     ]
    }
   ],
   "source": [
    "ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cmake to compile project (follow instructions: https://github.com/kpu/kenlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- The C compiler identification is GNU 9.4.0\n",
      "-- The CXX compiler identification is GNU 9.4.0\n",
      "-- Detecting C compiler ABI info\n",
      "-- Detecting C compiler ABI info - done\n",
      "-- Check for working C compiler: /usr/bin/cc - skipped\n",
      "-- Detecting C compile features\n",
      "-- Detecting C compile features - done\n",
      "-- Detecting CXX compiler ABI info\n",
      "-- Detecting CXX compiler ABI info - done\n",
      "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
      "-- Detecting CXX compile features\n",
      "-- Detecting CXX compile features - done\n",
      "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
      "-- Check if compiler accepts -pthread\n",
      "-- Check if compiler accepts -pthread - yes\n",
      "-- Found Threads: TRUE  \n",
      "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
      "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
      "-- Looking for BZ2_bzCompressInit\n",
      "-- Looking for BZ2_bzCompressInit - found\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
      "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
      "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.4\") \n",
      "-- Looking for clock_gettime in rt\n",
      "-- Looking for clock_gettime in rt - found\n",
      "-- Found OpenMP_C: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\") \n",
      "-- Found OpenMP: TRUE (found version \"4.5\")  \n",
      "-- Configuring done\n",
      "-- Generating done\n",
      "-- Build files have been written to: /home/geopar/projects/prep-lab/bash/kenlm/build\n",
      "[  1%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
      "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
      "[  4%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
      "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
      "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
      "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-to-string.cc.o\u001b[0m\n",
      "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/string-to-double.cc.o\u001b[0m\n",
      "[  9%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
      "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
      "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
      "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
      "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
      "[ 14%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
      "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
      "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
      "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
      "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
      "[ 19%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
      "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
      "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
      "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
      "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
      "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
      "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
      "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
      "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
      "[ 29%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
      "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
      "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
      "[ 32%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
      "[ 32%] Built target kenlm_util\n",
      "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
      "[ 34%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
      "[ 35%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
      "[ 36%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
      "[ 37%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
      "[ 38%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
      "[ 39%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
      "[ 40%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
      "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
      "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
      "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
      "[ 44%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
      "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
      "[ 46%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
      "[ 46%] Built target kenlm_filter\n",
      "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
      "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
      "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
      "[ 51%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
      "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
      "[ 52%] Built target probing_hash_table_benchmark\n",
      "[ 53%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
      "[ 54%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
      "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
      "[ 56%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
      "[ 57%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
      "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
      "[ 59%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
      "[ 59%] Built target kenlm\n",
      "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
      "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
      "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
      "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
      "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
      "[ 64%] Built target fragment\n",
      "[ 65%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
      "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
      "[ 66%] Built target build_binary\n",
      "[ 67%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
      "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
      "[ 68%] Built target query\n",
      "[ 69%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
      "[ 70%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
      "[ 71%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
      "[ 71%] Built target phrase_table_vocab\n",
      "[ 72%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/backoff_reunification.cc.o\u001b[0m\n",
      "[ 73%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/bounded_sequence_encoding.cc.o\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 75%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
      "[ 76%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/merge_probabilities.cc.o\u001b[0m\n",
      "[ 77%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/merge_vocab.cc.o\u001b[0m\n",
      "[ 78%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
      "[ 79%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/normalize.cc.o\u001b[0m\n",
      "[ 80%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
      "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
      "[ 81%] Built target kenlm_benchmark\n",
      "[ 82%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/pipeline.cc.o\u001b[0m\n",
      "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
      "[ 83%] Built target filter\n",
      "[ 84%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
      "[ 85%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/split_worker.cc.o\u001b[0m\n",
      "[ 86%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/tune_derivatives.cc.o\u001b[0m\n",
      "[ 87%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/tune_instances.cc.o\u001b[0m\n",
      "[ 88%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/tune_weights.cc.o\u001b[0m\n",
      "[ 89%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/kenlm_interpolate.dir/universal_vocab.cc.o\u001b[0m\n",
      "[ 90%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
      "[ 90%] Built target kenlm_builder\n",
      "[ 91%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
      "[ 92%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
      "[ 93%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_interpolate.a\u001b[0m\n",
      "[ 93%] Built target kenlm_interpolate\n",
      "[ 94%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/interpolate.dir/interpolate_main.cc.o\u001b[0m\n",
      "[ 95%] \u001b[32mBuilding CXX object lm/interpolate/CMakeFiles/streaming_example.dir/streaming_example_main.cc.o\u001b[0m\n",
      "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
      "[ 96%] Built target lmplz\n",
      "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
      "[ 97%] Built target count_ngrams\n",
      "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/interpolate\u001b[0m\n",
      "[ 98%] Built target interpolate\n",
      "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/streaming_example\u001b[0m\n",
      "[100%] Built target streaming_example\n"
     ]
    }
   ],
   "source": [
    "mkdir build\n",
    "cd build\n",
    "cmake ..\n",
    "make -j4\n",
    "# make install to install kenlm system-wide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous commands built the KenLM binaries inside the bin folder. Let's copy it in a more accessible directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 80\n",
      "-rw-rw-r-- 1 geopar geopar 22447 Mar 27 20:00 CMakeCache.txt\n",
      "drwxrwxr-x 7 geopar geopar  4096 Mar 27 20:01 CMakeFiles\n",
      "-rw-rw-r-- 1 geopar geopar 14626 Mar 27 20:00 Makefile\n",
      "drwxrwxr-x 2 geopar geopar  4096 Mar 27 20:01 bin\n",
      "-rw-rw-r-- 1 geopar geopar 14280 Mar 27 20:00 cmake_install.cmake\n",
      "-rw-rw-r-- 1 geopar geopar   701 Mar 27 20:00 kenlmConfig.cmake\n",
      "drwxrwxr-x 2 geopar geopar  4096 Mar 27 20:01 lib\n",
      "drwxrwxr-x 7 geopar geopar  4096 Mar 27 20:00 lm\n",
      "drwxrwxr-x 5 geopar geopar  4096 Mar 27 20:00 util\n"
     ]
    }
   ],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 220K\n",
      "drwxrwxr-x 5 geopar geopar 4.0K Mar 27 20:01  .\n",
      "drwxrwxr-x 6 geopar geopar 4.0K Mar 27 19:43  ..\n",
      "drwxrwxr-x 2 geopar geopar 4.0K Mar 27 19:59  .ipynb_checkpoints\n",
      "-rw-rw-r-- 1 geopar geopar  69K Mar 27 19:43 'Bash For Text Processing Introduction.ipynb'\n",
      "-rw-rw-r-- 1 geopar geopar 1.6K Mar 27 19:43  README.md\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:43  auto.csv\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:48  auto1.csv\n",
      "-rw-rw-r-- 1 geopar geopar  25K Mar 27 19:48  auto2.csv\n",
      "-rw-rw-r-- 1 geopar geopar 2.1K Mar 27 19:43  basic-commands.sh\n",
      "drwxrwxr-x 2 geopar geopar 4.0K Mar 27 20:01  bin\n",
      "-rwxrwxr-x 1 geopar geopar  543 Mar 27 19:43  chcase.py\n",
      "-rw-rw-r-- 1 geopar geopar 1.8K Mar 27 19:55  file-commands.sh\n",
      "-rw-r--r-- 1 geopar geopar  13K Mar 27 19:44  h_kernel.install\n",
      "-rw-rw-r-- 1 geopar geopar  454 Mar 27 19:43  install-openfst.sh\n",
      "-rw-rw-r-- 1 geopar geopar  535 Mar 27 19:43  install_openfst.sh\n",
      "drwxrwxr-x 9 geopar geopar 4.0K Mar 27 20:00  kenlm\n",
      "-rw-rw-r-- 1 geopar geopar  440 Mar 27 19:43  system-commands.sh\n"
     ]
    }
   ],
   "source": [
    "cp -r bin ../../  # 2 directories up\n",
    "cd ../../\n",
    "ls -lah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and preprocessing training corpus\n",
    "\n",
    "Let's get a book from project gutenberg and clean it up using bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-27 20:01:21--  http://www.gutenberg.org/cache/epub/345/pg345.txt\n",
      "Resolving www.gutenberg.org (www.gutenberg.org)... 152.19.134.47, 2610:28:3090:3000:0:bad:cafe:47\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://www.gutenberg.org/cache/epub/345/pg345.txt [following]\n",
      "--2023-03-27 20:01:21--  https://www.gutenberg.org/cache/epub/345/pg345.txt\n",
      "Connecting to www.gutenberg.org (www.gutenberg.org)|152.19.134.47|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 881691 (861K) [text/plain]\n",
      "Saving to: ‘data/dracula.txt’\n",
      "\n",
      "data/dracula.txt    100%[===================>] 861.03K  1.23MB/s    in 0.7s    \n",
      "\n",
      "2023-03-27 20:01:27 (1.23 MB/s) - ‘data/dracula.txt’ saved [881691/881691]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mkdir data\n",
    "wget -O data/dracula.txt http://www.gutenberg.org/cache/epub/345/pg345.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 872K\n",
      "drwxrwxr-x 2 geopar geopar 4.0K Mar 27 20:01 .\n",
      "drwxrwxr-x 6 geopar geopar 4.0K Mar 27 20:01 ..\n",
      "-rw-rw-r-- 1 geopar geopar 862K Mar 27 18:00 dracula.txt\n"
     ]
    }
   ],
   "source": [
    "ls -lah data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count the number of lines, words and characters using wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15869 data/dracula.txt\n"
     ]
    }
   ],
   "source": [
    "wc -l data/dracula.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can format column printing using awk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dracula.txt contains 15869 lines\n"
     ]
    }
   ],
   "source": [
    "wc -l data/dracula.txt | awk '{printf \"%s contains %s lines\\n\", $2, $1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dracula.txt contains 164459 words\n"
     ]
    }
   ],
   "source": [
    "wc -w data/dracula.txt | awk '{printf \"%s contains %s words\\n\", $2, $1}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dracula.txt contains 881691 characters\n"
     ]
    }
   ],
   "source": [
    "wc -c data/dracula.txt | awk '{printf \"%s contains %s characters\\n\", $2, $1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the first 250 lines using head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of Dracula, by Bram Stoker\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "\n",
      "Title: Dracula\n",
      "\n",
      "Author: Bram Stoker\n",
      "\n",
      "Release Date: October, 1995 [eBook #345]\n",
      "[Most recently updated: March 27, 2023]\n",
      "\n",
      "Language: English\n",
      "\n",
      "\n",
      "Produced by: Chuck Greif and the Online Distributed Proofreading Team\n",
      "\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK DRACULA ***\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                DRACULA\n",
      "\n",
      "                                  _by_\n",
      "\n",
      "                              Bram Stoker\n",
      "\n",
      "                        [Illustration: colophon]\n",
      "\n",
      "                                NEW YORK\n",
      "\n",
      "                            GROSSET & DUNLAP\n",
      "\n",
      "                              _Publishers_\n",
      "\n",
      "      Copyright, 1897, in the United States of America, according\n",
      "                   to Act of Congress, by Bram Stoker\n",
      "\n",
      "                        [_All rights reserved._]\n",
      "\n",
      "                      PRINTED IN THE UNITED STATES\n",
      "                                   AT\n",
      "               THE COUNTRY LIFE PRESS, GARDEN CITY, N.Y.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                   TO\n",
      "\n",
      "                             MY DEAR FRIEND\n",
      "\n",
      "                               HOMMY-BEG\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "CHAPTER I. Jonathan Harker’s Journal\n",
      "CHAPTER II. Jonathan Harker’s Journal\n",
      "CHAPTER III. Jonathan Harker’s Journal\n",
      "CHAPTER IV. Jonathan Harker’s Journal\n",
      "CHAPTER V. Letters—Lucy and Mina\n",
      "CHAPTER VI. Mina Murray’s Journal\n",
      "CHAPTER VII. Cutting from “The Dailygraph,” 8 August\n",
      "CHAPTER VIII. Mina Murray’s Journal\n",
      "CHAPTER IX. Mina Murray’s Journal\n",
      "CHAPTER X. Mina Murray’s Journal\n",
      "CHAPTER XI. Lucy Westenra’s Diary\n",
      "CHAPTER XII. Dr. Seward’s Diary\n",
      "CHAPTER XIII. Dr. Seward’s Diary\n",
      "CHAPTER XIV. Mina Harker’s Journal\n",
      "CHAPTER XV. Dr. Seward’s Diary\n",
      "CHAPTER XVI. Dr. Seward’s Diary\n",
      "CHAPTER XVII. Dr. Seward’s Diary\n",
      "CHAPTER XVIII. Dr. Seward’s Diary\n",
      "CHAPTER XIX. Jonathan Harker’s Journal\n",
      "CHAPTER XX. Jonathan Harker’s Journal\n",
      "CHAPTER XXI. Dr. Seward’s Diary\n",
      "CHAPTER XXII. Jonathan Harker’s Journal\n",
      "CHAPTER XXIII. Dr. Seward’s Diary\n",
      "CHAPTER XXIV. Dr. Seward’s Phonograph Diary, spoken by Van Helsing\n",
      "CHAPTER XXV. Dr. Seward’s Diary\n",
      "CHAPTER XXVI. Dr. Seward’s Diary\n",
      "CHAPTER XXVII. Mina Harker’s Journal\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How these papers have been placed in sequence will be made manifest in\n",
      "the reading of them. All needless matters have been eliminated, so that\n",
      "a history almost at variance with the possibilities of later-day belief\n",
      "may stand forth as simple fact. There is throughout no statement of\n",
      "past things wherein memory may err, for all the records chosen are\n",
      "exactly contemporary, given from the standpoints and within the range\n",
      "of knowledge of those who made them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "DRACULA\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CHAPTER I\n",
      "\n",
      "JONATHAN HARKER'S JOURNAL\n",
      "\n",
      "(_Kept in shorthand._)\n",
      "\n",
      "\n",
      "_3 May. Bistritz._--Left Munich at 8:35 P. M., on 1st May, arriving at\n",
      "Vienna early next morning; should have arrived at 6:46, but train was an\n",
      "hour late. Buda-Pesth seems a wonderful place, from the glimpse which I\n",
      "got of it from the train and the little I could walk through the\n",
      "streets. I feared to go very far from the station, as we had arrived\n",
      "late and would start as near the correct time as possible. The\n",
      "impression I had was that we were leaving the West and entering the\n",
      "East; the most western of splendid bridges over the Danube, which is\n",
      "here of noble width and depth, took us among the traditions of Turkish\n",
      "rule.\n",
      "\n",
      "We left in pretty good time, and came after nightfall to Klausenburgh.\n",
      "Here I stopped for the night at the Hotel Royale. I had for dinner, or\n",
      "rather supper, a chicken done up some way with red pepper, which was\n",
      "very good but thirsty. (_Mem._, get recipe for Mina.) I asked the\n",
      "waiter, and he said it was called \"paprika hendl,\" and that, as it was a\n",
      "national dish, I should be able to get it anywhere along the\n",
      "Carpathians. I found my smattering of German very useful here; indeed, I\n",
      "don't know how I should be able to get on without it.\n",
      "\n",
      "Having had some time at my disposal when in London, I had visited the\n",
      "British Museum, and made search among the books and maps in the library\n",
      "regarding Transylvania; it had struck me that some foreknowledge of the\n",
      "country could hardly fail to have some importance in dealing with a\n",
      "nobleman of that country. I find that the district he named is in the\n",
      "extreme east of the country, just on the borders of three states,\n",
      "Transylvania, Moldavia and Bukovina, in the midst of the Carpathian\n",
      "mountains; one of the wildest and least known portions of Europe. I was\n",
      "not able to light on any map or work giving the exact locality of the\n",
      "Castle Dracula, as there are no maps of this country as yet to compare\n",
      "with our own Ordnance Survey maps; but I found that Bistritz, the post\n",
      "town named by Count Dracula, is a fairly well-known place. I shall enter\n",
      "here some of my notes, as they may refresh my memory when I talk over my\n",
      "travels with Mina.\n",
      "\n",
      "In the population of Transylvania there are four distinct nationalities:\n",
      "Saxons in the South, and mixed with them the Wallachs, who are the\n",
      "descendants of the Dacians; Magyars in the West, and Szekelys in the\n",
      "East and North. I am going among the latter, who claim to be descended\n",
      "from Attila and the Huns. This may be so, for when the Magyars conquered\n",
      "the country in the eleventh century they found the Huns settled in it. I\n",
      "read that every known superstition in the world is gathered into the\n",
      "horseshoe of the Carpathians, as if it were the centre of some sort of\n",
      "imaginative whirlpool; if so my stay may be very interesting. (_Mem._, I\n",
      "must ask the Count all about them.)\n",
      "\n",
      "I did not sleep well, though my bed was comfortable enough, for I had\n",
      "all sorts of queer dreams. There was a dog howling all night under my\n",
      "window, which may have had something to do with it; or it may have been\n",
      "the paprika, for I had to drink up all the water in my carafe, and was\n",
      "still thirsty. Towards morning I slept and was wakened by the continuous\n",
      "knocking at my door, so I guess I must have been sleeping soundly then.\n",
      "I had for breakfast more paprika, and a sort of porridge of maize flour\n",
      "which they said was \"mamaliga,\" and egg-plant stuffed with forcemeat, a\n",
      "very excellent dish, which they call \"impletata.\" (_Mem._, get recipe\n",
      "for this also.) I had to hurry breakfast, for the train started a little\n",
      "before eight, or rather it ought to have done so, for after rushing to\n",
      "the station at 7:30 I had to sit in the carriage for more than an hour\n",
      "before we began to move. It seems to me that the further east you go the\n",
      "more unpunctual are the trains. What ought they to be in China?\n",
      "\n",
      "All day long we seemed to dawdle through a country which was full of\n",
      "beauty of every kind. Sometimes we saw little towns or castles on the\n",
      "top of steep hills such as we see in old missals; sometimes we ran by\n",
      "rivers and streams which seemed from the wide stony margin on each side\n",
      "of them to be subject to great floods. It takes a lot of water, and\n",
      "running strong, to sweep the outside edge of a river clear. At every\n",
      "station there were groups of people, sometimes crowds, and in all sorts\n",
      "of attire. Some of them were just like the peasants at home or those I\n",
      "saw coming through France and Germany, with short jackets and round hats\n",
      "and home-made trousers; but others were very picturesque. The women\n",
      "looked pretty, except when you got near them, but they were very clumsy\n",
      "about the waist. They had all full white sleeves of some kind or other,\n",
      "and most of them had big belts with a lot of strips of something\n",
      "fluttering from them like the dresses in a ballet, but of course there\n",
      "were petticoats under them. The strangest figures we saw were the\n",
      "Slovaks, who were more barbarian than the rest, with their big cow-boy\n",
      "hats, great baggy dirty-white trousers, white linen shirts, and enormous\n",
      "heavy leather belts, nearly a foot wide, all studded over with brass\n",
      "nails. They wore high boots, with their trousers tucked into them, and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "had long black hair and heavy black moustaches. They are very\n",
      "picturesque, but do not look prepossessing. On the stage they would be\n",
      "set down at once as some old Oriental band of brigands. They are,\n",
      "however, I am told, very harmless and rather wanting in natural\n",
      "self-assertion.\n",
      "\n",
      "It was on the dark side of twilight when we got to Bistritz, which is a\n",
      "very interesting old place. Being practically on the frontier--for the\n",
      "Borgo Pass leads from it into Bukovina--it has had a very stormy\n",
      "existence, and it certainly shows marks of it. Fifty years ago a series\n",
      "of great fires took place, which made terrible havoc on five separate\n",
      "occasions. At the very beginning of the seventeenth century it underwent\n",
      "a siege of three weeks and lost 13,000 people, the casualties of war\n",
      "proper being assisted by famine and disease.\n",
      "\n",
      "Count Dracula had directed me to go to the Golden Krone Hotel, which I\n",
      "found, to my great delight, to be thoroughly old-fashioned, for of\n",
      "course I wanted to see all I could of the ways of the country. I was\n",
      "evidently expected, for when I got near the door I faced a\n",
      "cheery-looking elderly woman in the usual peasant dress--white\n",
      "undergarment with long double apron, front, and back, of coloured stuff\n",
      "fitting almost too tight for modesty. When I came close she bowed and\n",
      "said, \"The Herr Englishman?\" \"Yes,\" I said, \"Jonathan Harker.\" She\n",
      "smiled, and gave some message to an elderly man in white shirt-sleeves,\n",
      "who had followed her to the door. He went, but immediately returned with\n",
      "a letter:--\n",
      "\n",
      "     \"My Friend.--Welcome to the Carpathians. I am anxiously expecting\n",
      "     you. Sleep well to-night. At three to-morrow the diligence will\n",
      "     start for Bukovina; a place on it is kept for you. At the Borgo\n",
      "     Pass my carriage will await you and will bring you to me. I trust\n",
      "     that your journey from London has been a happy one, and that you\n",
      "     will enjoy your stay in my beautiful land.\n",
      "\n",
      "\"Your friend,\n",
      "\n",
      "\"DRACULA.\"\n",
      "\n",
      "\n",
      "_4 May._--I found that my landlord had got a letter from the Count,\n",
      "directing him to secure the best place on the coach for me; but on\n",
      "making inquiries as to details he seemed somewhat reticent, and\n",
      "pretended that he could not understand my German. This could not be\n",
      "true, because up to then he had understood it perfectly; at least, he\n",
      "answered my questions exactly as if he did. He and his wife, the old\n",
      "lady who had received me, looked at each other in a frightened sort of\n",
      "way. He mumbled out that the money had been sent in a letter, and that\n",
      "was all he knew. When I asked him if he knew Count Dracula, and could\n",
      "tell me anything of his castle, both he and his wife crossed themselves,\n",
      "and, saying that they knew nothing at all, simply refused to speak\n",
      "further. It was so near the time of starting that I had no time to ask\n",
      "any one else, for it was all very mysterious and not by any means\n"
     ]
    }
   ],
   "source": [
    "head -250 data/dracula.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the first 200 lines contain project gutenberg specific text and the tableof contents. We can remove these using sed. Then we inspect the new file using head and wc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set down at once as some old Oriental band of brigands. They are,\n",
      "however, I am told, very harmless and rather wanting in natural\n",
      "self-assertion.\n",
      "\n",
      "It was on the dark side of twilight when we got to Bistritz, which is a\n",
      "very interesting old place. Being practically on the frontier--for the\n",
      "Borgo Pass leads from it into Bukovina--it has had a very stormy\n",
      "existence, and it certainly shows marks of it. Fifty years ago a series\n",
      "of great fires took place, which made terrible havoc on five separate\n",
      "occasions. At the very beginning of the seventeenth century it underwent\n"
     ]
    }
   ],
   "source": [
    "sed -e \"1,200d\" data/dracula.txt > data/dracula1.txt\n",
    "head data/dracula1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dracula1.txt contains 15669 lines, 163078 words and 873129 characters\n"
     ]
    }
   ],
   "source": [
    "wc data/dracula1.txt | \\\n",
    "    awk '{\n",
    "    printf \"%s contains %s lines, %s words and %s characters\\n\", $4, $1, $2, $3\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also remove all empty lines using sed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/dracula2.txt contains 13242 lines, 163078 words and 868275 characters\n"
     ]
    }
   ],
   "source": [
    "sed -r '/^\\s*$/d' data/dracula1.txt > data/dracula2.txt\n",
    "\n",
    "wc data/dracula2.txt | \\\n",
    "    awk '{\n",
    "    printf \"%s contains %s lines, %s words and %s characters\\n\", $4, $1, $2, $3\n",
    "    }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert all characters to lowercase using tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set down at once as some old oriental band of brigands. they are,\n",
      "however, i am told, very harmless and rather wanting in natural\n",
      "self-assertion.\n",
      "it was on the dark side of twilight when we got to bistritz, which is a\n",
      "very interesting old place. being practically on the frontier--for the\n",
      "borgo pass leads from it into bukovina--it has had a very stormy\n",
      "existence, and it certainly shows marks of it. fifty years ago a series\n",
      "of great fires took place, which made terrible havoc on five separate\n",
      "occasions. at the very beginning of the seventeenth century it underwent\n",
      "a siege of three weeks and lost 13,000 people, the casualties of war\n"
     ]
    }
   ],
   "source": [
    "tr A-Z a-z <data/dracula2.txt >data/dracula3.txt\n",
    "head data/dracula3.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And remove punctuation and numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set down at once as some old oriental band of brigands they are\n",
      "however i am told very harmless and rather wanting in natural\n",
      "selfassertion\n",
      "it was on the dark side of twilight when we got to bistritz which is a\n",
      "very interesting old place being practically on the frontierfor the\n",
      "borgo pass leads from it into bukovinait has had a very stormy\n",
      "existence and it certainly shows marks of it fifty years ago a series\n",
      "of great fires took place which made terrible havoc on five separate\n",
      "occasions at the very beginning of the seventeenth century it underwent\n",
      "a siege of three weeks and lost  people the casualties of war\n"
     ]
    }
   ],
   "source": [
    "cat data/dracula3.txt | tr -d [:punct:] | tr -d [:digit:] > data/dracula4.txt\n",
    "head data/dracula4.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can perform a word frequency analysis using uniq and sort.\n",
    "First we need to substitute spaces with newlines and then sort them to group the same words together. Uniq then will count consecutive lines that are the same and print word frequencies. We reverse sort the result to print most frequent words first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sed -r 's/\\s+/\\n/g' data/dracula4.txt | \\  # Replace spaces with new lines\n",
    "#     awk 'NF' | \\  # another way to remove empty lines\n",
    "#     sort | \\  # alphabetical sort\n",
    "#     uniq -c | \\ # word count \n",
    "#     sort -nr | \\ # reverse numeric sort\n",
    "#     awk '{$1=$1; print}' | \\  # strip leading and trailing whitespace\n",
    "#     awk 'BEGIN { OFS=\"\\t\" } {print $2,$1}' > data/wordcount.txt  # reverse columns\n",
    "    \n",
    "    \n",
    "sed -r 's/\\s+/\\n/g' data/dracula4.txt | \\\n",
    "    awk 'NF' | \\\n",
    "    sort | \\\n",
    "    uniq -c | \\\n",
    "    sort -nr | \\\n",
    "    awk '{$1=$1; print}' | \\\n",
    "    awk 'BEGIN { OFS=\" \"  } {print $2,$1}' > data/wordcount.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 7963\n",
      "and 5860\n",
      "i 4680\n",
      "to 4519\n",
      "of 3696\n",
      "a 2946\n",
      "he 2541\n",
      "in 2537\n",
      "that 2445\n",
      "it 2124\n"
     ]
    }
   ],
   "source": [
    "head data/wordcount.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even create a histogram of word counts using a simple python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "function histogram {\n",
    "python3 -c 'import sys\n",
    "for line in sys.stdin:\n",
    "  data, width = line.split()\n",
    "  print(\"{:<15}{:=<{width}}\".format(data, \"\", width=int(int(width) / 75)))' # each = corresponds to a count of 75\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat data/wordcount.txt  | histogram > data/histogram.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the            ==========================================================================================================\n",
      "and            ==============================================================================\n",
      "i              ==============================================================\n",
      "to             ============================================================\n",
      "of             =================================================\n",
      "a              =======================================\n",
      "he             =================================\n",
      "in             =================================\n",
      "that           ================================\n",
      "it             ============================\n",
      "was            ========================\n",
      "as             ====================\n",
      "we             ====================\n",
      "for            ====================\n",
      "is             ====================\n",
      "his            ===================\n",
      "you            ===================\n",
      "me             ===================\n",
      "not            ==================\n",
      "with           =================\n",
      "my             ================\n",
      "all            ===============\n",
      "be             ==============\n",
      "so             ==============\n",
      "at             ==============\n",
      "on             ==============\n",
      "but            ==============\n",
      "have           ==============\n",
      "her            ==============\n",
      "had            =============\n",
      "him            ============\n",
      "she            ==========\n",
      "when           ==========\n",
      "there          ==========\n",
      "this           ========\n",
      "which          ========\n",
      "if             ========\n",
      "from           ========\n",
      "are            =======\n",
      "said           =======\n",
      "then           =======\n",
      "were           =======\n",
      "by             =======\n",
      "or             ======\n",
      "could          ======\n",
      "one            ======\n",
      "do             ======\n",
      "no             ======\n",
      "us             ======\n",
      "what           ======\n",
      "will           ======\n",
      "them           ======\n",
      "they           ======\n",
      "must           =====\n",
      "up             =====\n",
      "some           =====\n",
      "out            =====\n",
      "would          =====\n",
      "shall          =====\n",
      "our            =====\n",
      "may            =====\n",
      "now            =====\n",
      "see            =====\n",
      "know           =====\n",
      "been           =====\n",
      "can            =====\n",
      "more           ====\n",
      "time           ====\n",
      "an             ====\n",
      "has            ====\n",
      "come           ====\n",
      "am             ====\n",
      "over           ====\n",
      "van            ====\n",
      "any            ====\n",
      "your           ====\n",
      "came           ====\n",
      "helsing        ===\n",
      "went           ===\n",
      "only           ===\n",
      "into           ===\n",
      "go             ===\n",
      "who            ===\n",
      "did            ===\n",
      "before         ===\n",
      "very           ===\n",
      "like           ===\n",
      "back           ===\n",
      "here           ===\n",
      "down           ===\n",
      "again          ===\n",
      "seemed         ===\n",
      "about          ===\n",
      "well           ===\n",
      "even           ===\n",
      "such           ===\n",
      "way            ===\n",
      "took           ==\n",
      "lucy           ==\n",
      "than           ==\n",
      "dear           ==\n",
      "think          ==\n",
      "where          ==\n",
      "much           ==\n",
      "good           ==\n",
      "their          ==\n",
      "man            ==\n",
      "how            ==\n",
      "though         ==\n",
      "saw            ==\n",
      "through        ==\n",
      "mina           ==\n",
      "too            ==\n",
      "hand           ==\n",
      "room           ==\n",
      "face           ==\n",
      "night          ==\n",
      "after          ==\n",
      "should         ==\n",
      "door           ==\n",
      "tell           ==\n",
      "made           ==\n",
      "poor           ==\n",
      "other          ==\n",
      "old            ==\n",
      "eyes           ==\n",
      "own            ==\n",
      "away           ==\n",
      "looked         ==\n",
      "work           ==\n",
      "friend         ==\n",
      "once           ==\n",
      "jonathan       ==\n",
      "great          ==\n",
      "sleep          ==\n",
      "dr             ==\n",
      "things         ==\n",
      "look           ==\n",
      "get            ==\n",
      "make           ==\n",
      "little         ==\n",
      "just           ==\n",
      "might          ==\n",
      "professor      ==\n",
      "day            ==\n",
      "got            ==\n",
      "its            ==\n",
      "found          ==\n",
      "yet            ==\n",
      "off            =\n",
      "god            =\n",
      "count          =\n",
      "take           =\n",
      "say            =\n",
      "long           =\n",
      "thought        =\n",
      "told           =\n",
      "men            =\n",
      "let            =\n",
      "life           =\n",
      "asked          =\n",
      "without        =\n",
      "something      =\n",
      "last           =\n",
      "till           =\n",
      "oh             =\n",
      "myself         =\n",
      "first          =\n",
      "fear           =\n",
      "place          =\n",
      "arthur         =\n",
      "ever           =\n",
      "house          =\n",
      "heart          =\n",
      "two            =\n",
      "never          =\n",
      "knew           =\n",
      "done           =\n",
      "himself        =\n",
      "these          =\n",
      "quite          =\n",
      "same           =\n",
      "want           =\n",
      "harker         =\n",
      "find           =\n",
      "still          =\n",
      "nothing        =\n",
      "began          =\n",
      "coming         =\n",
      "window         =\n",
      "put            =\n",
      "round          =\n",
      "head           =\n",
      "many           =\n",
      "hands          =\n",
      "however        =\n",
      "help           =\n",
      "right          =\n",
      "mr             =\n",
      "hear           =\n",
      "blood          =\n",
      "whilst         =\n",
      "mind           =\n",
      "open           =\n",
      "moment         =\n",
      "anything       =\n",
      "white          =\n",
      "keep           =\n",
      "full           =\n",
      "thing          =\n",
      "terrible       =\n",
      "left           =\n",
      "morning        =\n",
      "seen           =\n",
      "rest           =\n",
      "diary          =\n",
      "madam          =\n",
      "heard          =\n",
      "upon           =\n",
      "far            =\n",
      "why            =\n",
      "mrs            =\n",
      "strange        =\n",
      "cannot         =\n",
      "felt           =\n",
      "bed            =\n",
      "project        =\n",
      "few            =\n",
      "each           =\n",
      "tonight        =\n",
      "since          =\n",
      "godalming      =\n",
      "both           =\n",
      "turned         =\n",
      "dont           =\n",
      "read           =\n",
      "seward         =\n",
      "every          =\n",
      "being          =\n",
      "those          =\n",
      "light          =\n",
      "give           =\n",
      "alone          =\n",
      "quincey        =\n",
      "others         =\n",
      "opened         =\n",
      "love           =\n",
      "another        =\n",
      "stood          =\n",
      "new            =\n"
     ]
    }
   ],
   "source": [
    "head -250 data/histogram.txt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an n-gram language model\n",
    "\n",
    "Now we can use KenLM to train a 3-gram Language model on our preprocessed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Builds unpruned language models with modified Kneser-Ney smoothing.\n",
      "\n",
      "Please cite:\n",
      "@inproceedings{Heafield-estimate,\n",
      "  author = {Kenneth Heafield and Ivan Pouzyrevsky and Jonathan H. Clark and Philipp Koehn},\n",
      "  title = {Scalable Modified {Kneser-Ney} Language Model Estimation},\n",
      "  year = {2013},\n",
      "  month = {8},\n",
      "  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics},\n",
      "  address = {Sofia, Bulgaria},\n",
      "  url = {http://kheafield.com/professional/edinburgh/estimate\\_paper.pdf},\n",
      "}\n",
      "\n",
      "Provide the corpus on stdin.  The ARPA file will be written to stdout.  Order of\n",
      "the model (-o) is the only mandatory option.  As this is an on-disk program,\n",
      "setting the temporary file location (-T) and sorting memory (-S) is recommended.\n",
      "\n",
      "Memory sizes are specified like GNU sort: a number followed by a unit character.\n",
      "Valid units are % for percentage of memory (supported platforms only) and (in\n",
      "increasing powers of 1024): b, K, M, G, T, P, E, Z, Y.  Default is K (*1024).\n",
      "This machine has 134930374656 bytes of memory.\n",
      "\n",
      "Language model building options:\n",
      "  -h [ --help ]                         Show this help message\n",
      "  -o [ --order ] arg                    Order of the model\n",
      "  --interpolate_unigrams [=arg(=1)] (=1)\n",
      "                                        Interpolate the unigrams (default) as \n",
      "                                        opposed to giving lots of mass to <unk>\n",
      "                                        like SRI.  If you want SRI's behavior \n",
      "                                        with a large <unk> and the old lmplz \n",
      "                                        default, use --interpolate_unigrams 0.\n",
      "  --skip_symbols                        Treat <s>, </s>, and <unk> as \n",
      "                                        whitespace instead of throwing an \n",
      "                                        exception\n",
      "  -T [ --temp_prefix ] arg (=/tmp/)     Temporary file prefix\n",
      "  -S [ --memory ] arg (=80%)            Sorting memory\n",
      "  --minimum_block arg (=8K)             Minimum block size to allow\n",
      "  --sort_block arg (=64M)               Size of IO operations for sort \n",
      "                                        (determines arity)\n",
      "  --block_count arg (=2)                Block count (per order)\n",
      "  --vocab_estimate arg (=1000000)       Assume this vocabulary size for \n",
      "                                        purposes of calculating memory in step \n",
      "                                        1 (corpus count) and pre-sizing the \n",
      "                                        hash table\n",
      "  --vocab_pad arg (=0)                  If the vocabulary is smaller than this \n",
      "                                        value, pad with <unk> to reach this \n",
      "                                        size. Requires --interpolate_unigrams\n",
      "  --verbose_header                      Add a verbose header to the ARPA file \n",
      "                                        that includes information such as token\n",
      "                                        count, smoothing type, etc.\n",
      "  --text arg                            Read text from a file instead of stdin\n",
      "  --arpa arg                            Write ARPA to a file instead of stdout\n",
      "  --intermediate arg                    Write ngrams to intermediate files.  \n",
      "                                        Turns off ARPA output (which can be \n",
      "                                        reactivated by --arpa file).  Forces \n",
      "                                        --renumber on.\n",
      "  --renumber                            Renumber the vocabulary identifiers so \n",
      "                                        that they are monotone with the hash of\n",
      "                                        each string.  This is consistent with \n",
      "                                        the ordering used by the trie data \n",
      "                                        structure.\n",
      "  --collapse_values                     Collapse probability and backoff into a\n",
      "                                        single value, q that yields the same \n",
      "                                        sentence-level probabilities.  See \n",
      "                                        http://kheafield.com/professional/edinb\n",
      "                                        urgh/rest_paper.pdf for more details, \n",
      "                                        including a proof.\n",
      "  --prune arg                           Prune n-grams with count less than or \n",
      "                                        equal to the given threshold.  Specify \n",
      "                                        one value for each order i.e. 0 0 1 to \n",
      "                                        prune singleton trigrams and above.  \n",
      "                                        The sequence of values must be \n",
      "                                        non-decreasing and the last value \n",
      "                                        applies to any remaining orders. \n",
      "                                        Default is to not prune, which is \n",
      "                                        equivalent to --prune 0.\n",
      "  --limit_vocab_file arg                Read allowed vocabulary separated by \n",
      "                                        whitespace. N-grams that contain \n",
      "                                        vocabulary items not in this list will \n",
      "                                        be pruned. Can be combined with --prune\n",
      "                                        arg\n",
      "  --discount_fallback [=arg(=0.5 1 1.5)]\n",
      "                                        The closed-form estimate for Kneser-Ney\n",
      "                                        discounts does not work without \n",
      "                                        singletons or doubletons.  It can also \n",
      "                                        fail if these values are out of range. \n",
      "                                        This option falls back to \n",
      "                                        user-specified discounts when the \n",
      "                                        closed-form estimate fails.  Note that \n",
      "                                        this option is generally a bad idea: \n",
      "                                        you should deduplicate your corpus \n",
      "                                        instead.  However, class-based models \n",
      "                                        need custom discounts because they lack\n",
      "                                        singleton unigrams.  Provide up to \n",
      "                                        three discounts (for adjusted counts 1,\n",
      "                                        2, and 3+), which will be applied to \n",
      "                                        all orders where the closed-form \n",
      "                                        estimates fail.\n",
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "./bin/lmplz --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /home/geopar/projects/prep-lab/bash/data/dracula4.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 162112 types 10639\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:127668 2:37545799680 3:70398377984\n",
      "Statistics:\n",
      "1 10639 D1=0.6469 D2=0.97884 D3+=1.42672\n",
      "2 72095 D1=0.771123 D2=1.14955 D3+=1.3127\n",
      "3 132989 D1=0.882078 D2=1.25077 D3+=1.43877\n",
      "Memory estimate for binary LM:\n",
      "type      kB\n",
      "probing 4297 assuming -p 1.5\n",
      "probing 4761 assuming -r models -p 1.5\n",
      "trie    1816 without quantization\n",
      "trie    1032 assuming -q 8 -b 8 quantization \n",
      "trie    1726 assuming -a 22 array pointer compression\n",
      "trie     942 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:127668 2:1153520 3:2659780\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:127668 2:1153520 3:2659780\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:105562808 kB\tVmRSS:10476 kB\tRSSMax:24348416 kB\tuser:1.4394\tsys:6.01604\tCPU:7.45546\treal:7.46185\n"
     ]
    }
   ],
   "source": [
    "./bin/lmplz -o 3 <data/dracula4.txt > data/dracula.lm.arpa "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some 1-gram, 2-gram and 3-gram scores using grep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\u001b[01;31m\u001b[K1-grams\u001b[m\u001b[K:\n",
      "-4.8847647\t<unk>\t0\n",
      "0\t<s>\t-0.6120616\n",
      "-1.4304981\t</s>\t0\n",
      "-3.2397177\tset\t-0.25380522\n",
      "-2.8067012\tdown\t-0.46421018\n",
      "-2.1389773\tat\t-0.70574903\n",
      "-3.2611618\tonce\t-0.22900844\n",
      "-1.9901353\tas\t-0.77390605\n",
      "-2.652394\tsome\t-0.33119375\n",
      "-3.0619326\told\t-0.2307953\n",
      "\u001b[36m\u001b[K--\u001b[m\u001b[K\n",
      "\\\u001b[01;31m\u001b[K2-grams\u001b[m\u001b[K:\n",
      "-1.7209909\t<s> </s>\t0\n",
      "-1.2245114\tset </s>\t0\n",
      "-1.0669799\tdown </s>\t0\n",
      "-1.0208355\tat </s>\t0\n",
      "-1.2678803\tonce </s>\t0\n",
      "-1.1870296\tas </s>\t0\n",
      "-0.9969789\tsome </s>\t0\n",
      "-1.1066624\told </s>\t0\n",
      "-1.3913614\tband </s>\t0\n",
      "-1.0530057\tof </s>\t0\n",
      "\u001b[36m\u001b[K--\u001b[m\u001b[K\n",
      "\\\u001b[01;31m\u001b[K3-grams\u001b[m\u001b[K:\n",
      "-1.0877582\tas set </s>\n",
      "-0.95248246\tare set </s>\n",
      "-0.95248246\ti set </s>\n",
      "-0.95248246\tsun set </s>\n",
      "-1.2081404\tand down </s>\n",
      "-1.0713937\tit down </s>\n",
      "-0.71326697\tcould down </s>\n",
      "-1.0035524\tcame down </s>\n",
      "-1.2432148\twent down </s>\n",
      "-0.50769204\tlooked down </s>\n"
     ]
    }
   ],
   "source": [
    "cat data/dracula.lm.arpa | egrep \"1-grams|2-grams|3-grams\" -A10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use query to use the trained language model to score the perplexity of a sentence.\n",
    "Lower perplexity indicates a more probable sentence.\n",
    "\n",
    "Let's have the model score two possible endings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./bin/query: invalid option -- 'h'\n",
      "KenLM was compiled with maximum order 6.\n",
      "Usage: ./bin/query [-b] [-n] [-w] [-s] lm_file\n",
      "-b: Do not buffer output.\n",
      "-n: Do not wrap the input in <s> and </s>.\n",
      "-v summary|sentence|word: Print statistics at this level.\n",
      "   Can be used multiple times: -v summary -v sentence -v word\n",
      "-l lazy|populate|read|parallel: Load lazily, with populate, or malloc+read\n",
      "The default loading method is populate on Linux and read on others.\n",
      "\n",
      "Each word in the output is formatted as:\n",
      "  word=vocab_id ngram_length log10(p(word|context))\n",
      "where ngram_length is the length of n-gram matched.  A vocab_id of 0 indicates\n",
      "the unknown word. Sentence-level output includes log10 probability of the\n",
      "sentence and OOV count.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "./bin/query -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"harker and mina die a horrible death\" > data/bad_ending\n",
    "echo \"harker and mina live happily ever after\" > data/good_ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harker and mina die a horrible death\n",
      "Perplexity including OOVs:\t455.41437459451936\n"
     ]
    }
   ],
   "source": [
    "cat data/bad_ending\n",
    "./bin/query data/dracula.lm.arpa < data/bad_ending 2>&1| grep \"Perplexity\" | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "harker and mina live happily ever after\n",
      "Perplexity including OOVs:\t909.8785384403461\n"
     ]
    }
   ],
   "source": [
    "cat data/good_ending\n",
    "./bin/query data/dracula.lm.arpa < data/good_ending 2>&1| grep \"Perplexity\" | head -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
